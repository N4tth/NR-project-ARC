{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducci√≥n\n",
    "Este notebook aborda la clasificaci√≥n del n√∫mero de estrellas (1‚Äì5) asignadas a una rese√±a de Amazon a partir de su texto. El flujo general incluye: descarga del dataset, construcci√≥n del texto de entrada, particionado en conjuntos de entrenamiento/validaci√≥n/test, tokenizaci√≥n subword usando el tokenizador de XLM-R, definici√≥n de varias arquitecturas MLP basadas en un embedding entrenado desde cero, ejecuci√≥n de experimentos con distintos hiperpar√°metros, visualizaci√≥n de curvas y evaluaci√≥n con m√©tricas (accuracy y F1). A lo largo del documento se explican decisiones, supuestos y limitaciones.\n",
    "\n",
    "### Objetivo\n",
    "Predecir la etiqueta (n√∫mero de estrellas) optimizando la funci√≥n de p√©rdida de entrop√≠a cruzada y comparar arquitecturas de complejidad creciente.\n",
    "\n",
    "### Gu√≠a r√°pida de secciones\n",
    "1. Carga y exploraci√≥n del dataset\n",
    "2. Preprocesamiento del texto y creaci√≥n de etiquetas\n",
    "3. Conversi√≥n a `Dataset` de Hugging Face y splits\n",
    "4. Tokenizaci√≥n y preparaci√≥n de tensores\n",
    "5. Definici√≥n de arquitecturas MLP\n",
    "6. Entrenamiento y ejecuci√≥n de experimentos\n",
    "7. Visualizaci√≥n y m√©tricas adicionales\n",
    "8. Conclusiones y observaciones finales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MnLgRUkFGC3"
   },
   "outputs": [],
   "source": [
    "# Importaciones principales\n",
    "# En esta celda se cargan las librer√≠as necesarias para:\n",
    "# - Manipulaci√≥n de datos: pandas\n",
    "# - Descarga del dataset desde Kaggle: kagglehub\n",
    "# - Construcci√≥n y entrenamiento de modelos: PyTorch (torch, nn, optim)\n",
    "# - M√©tricas: accuracy_score, classification_report (sklearn)\n",
    "# - Visualizaci√≥n: matplotlib\n",
    "# - Tokenizaci√≥n y manejo de datasets: datasets (HF) y transformers (AutoTokenizer)\n",
    "\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y exploraci√≥n del dataset\n",
    "En esta secci√≥n se descarga el dataset de rese√±as de Amazon usando `kagglehub` y se carga el archivo `train.csv` en un DataFrame de pandas.\n",
    "\n",
    "### Objetivos\n",
    "- Verificar forma (n√∫mero de filas/columnas).\n",
    "- Inspeccionar nombres de columnas disponibles.\n",
    "- Observar primeras filas para entender estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar archivo\n",
    "file_path = kagglehub.dataset_download(\"mexwell/amazon-reviews-multi\")\n",
    "\n",
    "csv_path = file_path + \"/train.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path, encoding=\"latin-1\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columnas:\", df.columns)\n",
    "print(\"Primeras 5 filas:\\n\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de texto y creaci√≥n de etiquetas\n",
    "Se unifica el t√≠tulo y el cuerpo de la rese√±a en un √∫nico campo `text` para maximizar el contexto disponible. Adem√°s, se ajustan las etiquetas de estrellas para que comiencen en 0 (requisito de `CrossEntropyLoss`).\n",
    "\n",
    "### Detalles\n",
    "- `text = review_title + ' ' + review_body` con `fillna('')` para evitar valores nulos.\n",
    "- `labels = stars - 1` transforma el rango [1,5] a [0,4].\n",
    "- Se conservan solo columnas relevantes: `text`, `labels`, `language`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unificaci√≥n de t√≠tulo y cuerpo de la rese√±a + creaci√≥n de etiquetas normalizadas\n",
    "# Se genera un √∫nico campo de texto para aprovechar todo el contexto disponible.\n",
    "# Las etiquetas (n√∫mero de estrellas) se desplazan a rango 0..4, requerido por CrossEntropyLoss.\n",
    "# Se retienen √∫nicamente columnas necesarias para el modelo.\n",
    "# Nota: No se hace limpieza adicional (lowercase, eliminar signos, etc.) porque el tokenizador subword maneja vocab enriquecido.\n",
    "\n",
    "# Combina t√≠tulo y cuerpo (maneja NaN con cadenas vac√≠as)\n",
    "df[\"text\"] = df[\"review_title\"].fillna(\"\") + \" \" + df[\"review_body\"].fillna(\"\")\n",
    "# Ajusta etiquetas al rango 0-4\n",
    "df[\"labels\"] = df[\"stars\"] - 1\n",
    "# Subconjunto de columnas relevantes\n",
    "df = df[[\"text\", \"labels\", \"language\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversi√≥n a Dataset y particionado (train/validation/test)\n",
    "Se convierte el DataFrame a un `datasets.Dataset` de Hugging Face y se generan tres subconjuntos:\n",
    "- `train_dataset`: datos para entrenamiento.\n",
    "- `validation_dataset`: para monitorear generalizaci√≥n y aplicar early stopping.\n",
    "- `test_dataset`: evaluaci√≥n final.\n",
    "\n",
    "### Detalles\n",
    "- Primer split: 90% train + 10% test.\n",
    "- Segundo split: del train original se separa 10% para validaci√≥n (‚âà 9% total).\n",
    "- Semilla fija `seed=42` para reproducibilidad del particionado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversi√≥n del DataFrame a un objeto Dataset de Hugging Face\n",
    "# Esto facilita operaciones vectorizadas, mapeo y compatibilidad con DataLoader posteriormente.\n",
    "# Se crean dos niveles de splits: train/test y luego train/validation.\n",
    "\n",
    "#Conversi√≥n a hugging face dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Divisi√≥n del dataset en train y test (10% test)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "# Divisi√≥n adicional para generar validaci√≥n desde el train restante\n",
    "train_val = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Asignaci√≥n final de subconjuntos\n",
    "train_dataset = train_val[\"train\"]\n",
    "validation_dataset = train_val[\"test\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizaci√≥n y preparaci√≥n de tensores\n",
    "Se utiliza el tokenizador subword de `xlm-roberta-base` para segmentar texto multiling√ºe.\n",
    "\n",
    "### Par√°metros clave\n",
    "- `truncation=True`: corta rese√±as largas para no exceder `max_length`.\n",
    "- `padding=\"max_length\"`: fija longitud uniforme (facilita batching, pero introduce tokens PAD en rese√±as cortas).\n",
    "- `max_length=180`: compromiso entre cobertura contextual y coste computacional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizaci√≥n subword con el tokenizador de XLM-R\n",
    "model_name = \"xlm-roberta-base\"\n",
    "# Carga del tokenizador multiling√ºe\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Funci√≥n de preprocesamiento (mapeable sobre batches)\n",
    "def preprocess(batch):\n",
    "  return tokenizer(\n",
    "      batch[\"text\"],               # Lista de textos\n",
    "      truncation=True,              # Corta secuencias largas\n",
    "      padding=\"max_length\",        # Padding uniforme hasta max_length\n",
    "      max_length=180                # Longitud m√°xima (hiperpar√°metro manual)\n",
    "  )\n",
    "\n",
    "# Aplicaci√≥n vectorizada a cada subconjunto\n",
    "train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "validation_dataset = validation_dataset.map(preprocess, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Selecci√≥n de columnas y conversi√≥n a tensores de PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "validation_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definici√≥n de arquitecturas MLP basadas en embeddings\n",
    "Se definen tres variantes de redes neuronales densas (MLP) que comparten:\n",
    "- Capa de embeddings entrenada desde cero (dimensi√≥n configurable).\n",
    "- Pooling por promedio sobre la secuencia.\n",
    "- Capas densas intermedias con activaci√≥n ReLU y Dropout (0.2).\n",
    "- Capa final lineal a 5 clases (sin softmax expl√≠cito porque `CrossEntropyLoss` lo combina internamente).\n",
    "\n",
    "### Diferencias principales\n",
    "- Simple: Menor profundidad y capacidad.\n",
    "- Intermedia: Tres capas ocultas progresivas.\n",
    "- Avanzada: Cuatro capas ocultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definici√≥n de tres arquitecturas MLP de distinta profundidad.\n",
    "# Patr√≥n com√∫n:\n",
    "# 1. Embedding: convierte IDs de subwords en vectores densos entrenables.\n",
    "# 2. Pooling (mean): colapsa dimensi√≥n secuencial -> vector fijo.\n",
    "# 3. Capas densas con ReLU + Dropout (0.2) para introducir no linealidad y regularizaci√≥n.\n",
    "# 4. Capa de salida: logits para 5 clases (estrellas 0..4).\n",
    "# Nota: se usa CrossEntropyLoss posteriormente, por eso no hay Softmax aqu√≠.\n",
    "\n",
    "class RedNeuronalSimple(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim1=128, hidden_dim2=64, output_dim=5, pad_idx=0):\n",
    "        super(RedNeuronalSimple, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Capa oculta 1\n",
    "        self.capa_oculta_1 = nn.Linear(embedding_dim, hidden_dim1)\n",
    "        self.activacion_1 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa oculta 2\n",
    "        self.capa_oculta_2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.activacion_2 = nn.ReLU()\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.capa_salida = nn.Linear(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)        \n",
    "        pooled = embedded.mean(dim=1)             \n",
    "        x = self.capa_oculta_1(pooled)\n",
    "        x = self.activacion_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.capa_oculta_2(x)\n",
    "        x = self.activacion_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.capa_salida(x)                   \n",
    "        return x\n",
    "\n",
    "class RedNeuronalIntermedia(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim1=256, hidden_dim2=128, hidden_dim3=64, output_dim=5, pad_idx=0):\n",
    "        super(RedNeuronalIntermedia, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Capa oculta 1\n",
    "        self.capa_oculta_1 = nn.Linear(embedding_dim, hidden_dim1)\n",
    "        self.activacion_1 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa oculta 2\n",
    "        self.capa_oculta_2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.activacion_2 = nn.ReLU()\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa oculta 3\n",
    "        self.capa_oculta_3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.activacion_3 = nn.ReLU()\n",
    "        self.dropout_3 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.capa_salida = nn.Linear(hidden_dim3, output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        pooled = embedded.mean(dim=1)\n",
    "        x = self.capa_oculta_1(pooled)\n",
    "        x = self.activacion_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.capa_oculta_2(x)\n",
    "        x = self.activacion_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.capa_oculta_3(x)\n",
    "        x = self.activacion_3(x)\n",
    "        x = self.dropout_3(x)\n",
    "        x = self.capa_salida(x)\n",
    "        return x\n",
    "\n",
    "class RedNeuronalAvanzada(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim1=512, hidden_dim2=256, hidden_dim3=128, hidden_dim4=64, output_dim=5, pad_idx=0):\n",
    "        super(RedNeuronalAvanzada, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        # Capa oculta 1\n",
    "        self.capa_oculta_1 = nn.Linear(embedding_dim, hidden_dim1)\n",
    "        self.activacion_1 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa oculta 2\n",
    "        self.capa_oculta_2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.activacion_2 = nn.ReLU()\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa oculta 3\n",
    "        self.capa_oculta_3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.activacion_3 = nn.ReLU()\n",
    "        self.dropout_3 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa oculta 4\n",
    "        self.capa_oculta_4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        self.activacion_4 = nn.ReLU()\n",
    "        self.dropout_4 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.capa_salida = nn.Linear(hidden_dim4, output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        pooled = embedded.mean(dim=1)\n",
    "        x = self.capa_oculta_1(pooled)\n",
    "        x = self.activacion_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.capa_oculta_2(x)\n",
    "        x = self.activacion_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.capa_oculta_3(x)\n",
    "        x = self.activacion_3(x)\n",
    "        x = self.dropout_3(x)\n",
    "        x = self.capa_oculta_4(x)\n",
    "        x = self.activacion_4(x)\n",
    "        x = self.dropout_4(x)\n",
    "        x = self.capa_salida(x)\n",
    "        return x\n",
    "\n",
    "def setup_training(model, learning_rate=0.001):\n",
    "    # Configura Adam como optimizador y CrossEntropy para clasificaci√≥n multiclase.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return optimizer, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de entrenamiento y evaluaci√≥n (versi√≥n 1)\n",
    "Se definen helpers para entrenar por √©poca, evaluar en validaci√≥n y un loop principal con early stopping basado en *validation accuracy*.\n",
    "\n",
    "### Componentes\n",
    "- `train_epoch`: calcula p√©rdida media y accuracy en entrenamiento.\n",
    "- `evaluate_model`: calcula p√©rdida y accuracy en validaci√≥n sin gradientes.\n",
    "- `train_model`: orquesta m√∫ltiples √©pocas, aplica early stopping proporcional al n√∫mero total de √©pocas, y guarda el mejor estado del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Dispositivo de entrenamiento: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuraci√≥n del dispositivo (GPU si disponible, de lo contrario CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîß Dispositivo de entrenamiento: {device}\")\n",
    "\n",
    "# Funci√≥n de entrenamiento por √©poca\n",
    "# Recorre todos los batches, acumula p√©rdida y calcula accuracy acumulado.\n",
    "# Nota: El embedding se entrena desde cero; grandes batches pueden suavizar gradientes.\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"Entrena el modelo por una √©poca y devuelve (loss_media, accuracy).\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Datos del batch (input_ids y labels ya en tensores Torch)\n",
    "        inputs = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()              # Limpia gradientes previos\n",
    "        outputs = model(inputs)            # Forward: logits [batch, num_clases]\n",
    "        loss = criterion(outputs, labels)  # CrossEntropyLoss (logits + labels enteros)\n",
    "\n",
    "        loss.backward()                    # Backpropagation\n",
    "        optimizer.step()                   # Actualiza par√°metros\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)   # Clase m√°s probable\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "# Funci√≥n de evaluaci√≥n (sin actualizaci√≥n de gradientes)\n",
    "# Calcula p√©rdida y accuracy en validaci√≥n para monitorear generalizaci√≥n.\n",
    "def evaluate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"Eval√∫a el modelo en el conjunto de validaci√≥n y devuelve (loss_media, accuracy).\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "# Loop principal de entrenamiento con early stopping basado en validation accuracy.\n",
    "# Guarda el mejor estado (mayor val_accuracy) y detiene si no mejora tras 'patience' √©pocas.\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs, device, model_name, exp_name):\n",
    "    \"\"\"Entrenamiento multiepoch con early stopping. Devuelve m√©tricas y mejor modelo.\"\"\"\n",
    "    print(f\"\\nüöÄ Iniciando entrenamiento - {model_name} - {exp_name}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    patience = max(1, epochs // 4)   # Paciencia proporcional al n√∫mero m√°ximo de √©pocas\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nüìä √âpoca {epoch + 1}/{epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"‚è≥ Sin mejora en val_acc. Paciencia: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"‚èπÔ∏è Early Stopping activado en √©poca {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f\"\\n‚úÖ Entrenamiento completado!\")\n",
    "    print(f\"üèÜ Mejor modelo: √âpoca {best_epoch} con Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'best_val_accuracy': best_val_accuracy,\n",
    "        'best_epoch': best_epoch,\n",
    "        'stopped_epoch': epoch +1,\n",
    "        'model': model\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecuci√≥n de experimentos (versi√≥n 1)\n",
    "Se itera sobre las tres arquitecturas definidas y dos configuraciones de hiperpar√°metros.\n",
    "\n",
    "### Experimentos\n",
    "- `exp_1`: lr=0.001, epochs=100, batch=1024\n",
    "- `exp_2`: lr=0.0005, epochs=500, batch=2048\n",
    "\n",
    "### Objetivo\n",
    "Comparar el impacto de profundidad y n√∫mero de √©pocas / tama√±o de batch sobre la accuracy de validaci√≥n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ INICIANDO EXPERIMENTOS DE REDES NEURONALES\n",
      "================================================================================\n",
      "\n",
      "üî¨ MODELO: RedNeuronalSimple\n",
      "============================================================\n",
      "\n",
      "üìã Experimento: exp_1\n",
      "   Par√°metros: {'learning_rate': 0.001, 'epochs': 1, 'batch_size': 1024}\n",
      "\n",
      "üöÄ Iniciando entrenamiento - RedNeuronalSimple - exp_1\n",
      "======================================================================\n",
      "\n",
      "üìä √âpoca 1/1\n",
      "--------------------------------------------------\n",
      "Train Loss: 63.8235 | Train Acc: 20.01%\n",
      "Val Loss: 1.6094   | Val Acc: 20.03%\n",
      "üåü ¬°Nuevo mejor modelo!\n",
      "\n",
      "‚úÖ Entrenamiento completado!\n",
      "üèÜ Mejor modelo: √âpoca 1 con Val Accuracy: 20.03%\n",
      "\n",
      "üî¨ MODELO: RedNeuronalIntermedia\n",
      "============================================================\n",
      "\n",
      "üìã Experimento: exp_1\n",
      "   Par√°metros: {'learning_rate': 0.001, 'epochs': 1, 'batch_size': 1024}\n",
      "\n",
      "üöÄ Iniciando entrenamiento - RedNeuronalIntermedia - exp_1\n",
      "======================================================================\n",
      "\n",
      "üìä √âpoca 1/1\n",
      "--------------------------------------------------\n",
      "Train Loss: 63.8235 | Train Acc: 20.01%\n",
      "Val Loss: 1.6094   | Val Acc: 20.03%\n",
      "üåü ¬°Nuevo mejor modelo!\n",
      "\n",
      "‚úÖ Entrenamiento completado!\n",
      "üèÜ Mejor modelo: √âpoca 1 con Val Accuracy: 20.03%\n",
      "\n",
      "üî¨ MODELO: RedNeuronalIntermedia\n",
      "============================================================\n",
      "\n",
      "üìã Experimento: exp_1\n",
      "   Par√°metros: {'learning_rate': 0.001, 'epochs': 1, 'batch_size': 1024}\n",
      "\n",
      "üöÄ Iniciando entrenamiento - RedNeuronalIntermedia - exp_1\n",
      "======================================================================\n",
      "\n",
      "üìä √âpoca 1/1\n",
      "--------------------------------------------------\n",
      "Train Loss: 23.4727 | Train Acc: 20.07%\n",
      "Val Loss: 1.6095   | Val Acc: 19.93%\n",
      "üåü ¬°Nuevo mejor modelo!\n",
      "\n",
      "‚úÖ Entrenamiento completado!\n",
      "üèÜ Mejor modelo: √âpoca 1 con Val Accuracy: 19.93%\n",
      "\n",
      "üî¨ MODELO: RedNeuronalAvanzada\n",
      "============================================================\n",
      "\n",
      "üìã Experimento: exp_1\n",
      "   Par√°metros: {'learning_rate': 0.001, 'epochs': 1, 'batch_size': 1024}\n",
      "\n",
      "üöÄ Iniciando entrenamiento - RedNeuronalAvanzada - exp_1\n",
      "======================================================================\n",
      "\n",
      "üìä √âpoca 1/1\n",
      "--------------------------------------------------\n",
      "Train Loss: 23.4727 | Train Acc: 20.07%\n",
      "Val Loss: 1.6095   | Val Acc: 19.93%\n",
      "üåü ¬°Nuevo mejor modelo!\n",
      "\n",
      "‚úÖ Entrenamiento completado!\n",
      "üèÜ Mejor modelo: √âpoca 1 con Val Accuracy: 19.93%\n",
      "\n",
      "üî¨ MODELO: RedNeuronalAvanzada\n",
      "============================================================\n",
      "\n",
      "üìã Experimento: exp_1\n",
      "   Par√°metros: {'learning_rate': 0.001, 'epochs': 1, 'batch_size': 1024}\n",
      "\n",
      "üöÄ Iniciando entrenamiento - RedNeuronalAvanzada - exp_1\n",
      "======================================================================\n",
      "\n",
      "üìä √âpoca 1/1\n",
      "--------------------------------------------------\n",
      "Train Loss: 11.5856 | Train Acc: 19.97%\n",
      "Val Loss: 1.6094   | Val Acc: 20.03%\n",
      "üåü ¬°Nuevo mejor modelo!\n",
      "\n",
      "‚úÖ Entrenamiento completado!\n",
      "üèÜ Mejor modelo: √âpoca 1 con Val Accuracy: 20.03%\n",
      "\n",
      "üéâ TODOS LOS EXPERIMENTOS COMPLETADOS\n",
      "================================================================================\n",
      "Train Loss: 11.5856 | Train Acc: 19.97%\n",
      "Val Loss: 1.6094   | Val Acc: 20.03%\n",
      "üåü ¬°Nuevo mejor modelo!\n",
      "\n",
      "‚úÖ Entrenamiento completado!\n",
      "üèÜ Mejor modelo: √âpoca 1 con Val Accuracy: 20.03%\n",
      "\n",
      "üéâ TODOS LOS EXPERIMENTOS COMPLETADOS\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Bucle maestro de experimentos sobre tres arquitecturas y dos configuraciones de hiperpar√°metros.\n",
    "# Nota: Este bloque se replica m√°s adelante con ligera variaci√≥n; mantener solo una versi√≥n en producci√≥n facilitar√≠a mantenimiento.\n",
    "\n",
    "# EJECUCI√ìN DE TODOS LOS EXPERIMENTOS\n",
    "print(\"üéØ INICIANDO EXPERIMENTOS DE REDES NEURONALES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Diccionario de clases de modelos\n",
    "models_dict = {\n",
    "    'RedNeuronalSimple': RedNeuronalSimple,\n",
    "    'RedNeuronalIntermedia': RedNeuronalIntermedia,\n",
    "    'RedNeuronalAvanzada': RedNeuronalAvanzada\n",
    "}\n",
    "\n",
    "# Hiperpar√°metros por experimento\n",
    "experiments = {\n",
    "    \"exp_1\": {\"learning_rate\": 0.001, \"epochs\": 100, \"batch_size\": 1024},\n",
    "    \"exp_2\": {\"learning_rate\": 0.0005, \"epochs\": 500, \"batch_size\": 2048},\n",
    "}\n",
    "\n",
    "# Tama√±o del vocabulario del tokenizador (embedding entrenado desde cero)\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "embedding_dim = 180              # Dimensi√≥n de embedding arbitraria elegida\n",
    "output_dim = 5                   # N√∫mero de clases (estrellas 0..4)\n",
    "pad_idx = tokenizer.pad_token_id # √çndice de padding\n",
    "\n",
    "all_results = {}                 # Almacenar√° m√©tricas y mejores √©pocas por modelo/experimento\n",
    "\n",
    "for model_name, ModelClass in models_dict.items():\n",
    "    print(f\"\\nüî¨ MODELO: {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    model_results = {}\n",
    "\n",
    "    for exp_name, params in experiments.items():\n",
    "        print(f\"\\nüìã Experimento: {exp_name}\")\n",
    "        print(f\"   Par√°metros: {params}\")\n",
    "\n",
    "        # DataLoaders con batch_size espec√≠fico\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                                 batch_size=params[\"batch_size\"],\n",
    "                                 shuffle=True)\n",
    "        val_loader = DataLoader(validation_dataset,\n",
    "                               batch_size=params[\"batch_size\"],\n",
    "                               shuffle=False)\n",
    "\n",
    "        # Instanciaci√≥n (red simple pasa hidden_dim expl√≠cito)\n",
    "        if model_name == \"RedNeuronalSimple\":\n",
    "            model = ModelClass(vocab_size, embedding_dim, output_dim=output_dim, pad_idx=pad_idx).to(device)\n",
    "        elif model_name == \"RedNeuronalIntermedia\":\n",
    "            model = ModelClass(vocab_size, embedding_dim, output_dim=output_dim, pad_idx=pad_idx).to(device)\n",
    "        elif model_name == \"RedNeuronalAvanzada\":\n",
    "            model = ModelClass(vocab_size, embedding_dim, output_dim=output_dim, pad_idx=pad_idx).to(device)\n",
    "\n",
    "        optimizer, criterion = setup_training(model, learning_rate=params[\"learning_rate\"])\n",
    "\n",
    "        # Entrenamiento y captura de resultados\n",
    "        results = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            epochs=params[\"epochs\"],\n",
    "            device=device,\n",
    "            model_name=model_name,\n",
    "            exp_name=exp_name\n",
    "        )\n",
    "\n",
    "        results['params'] = params\n",
    "        model_results[exp_name] = results\n",
    "\n",
    "    all_results[model_name] = model_results\n",
    "\n",
    "print(f\"\\nüéâ TODOS LOS EXPERIMENTOS COMPLETADOS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almacenamiento de resultados (versi√≥n 1)\n",
    "Se almacenan los resultados en archivos csv para cada modelo y los experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUARDAR RESULTADOS EN ARCHIVOS CSV\n",
    "print(\"\\nüíæ GUARDANDO RESULTADOS EN ARCHIVOS CSV\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import os\n",
    "\n",
    "# Crear directorio de resultados si no existe\n",
    "results_dir = \"resultados\"\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    print(f\"üìÅ Directorio '{results_dir}' creado\")\n",
    "\n",
    "# Mapeo de nombres de modelos a nombres de archivos\n",
    "file_mapping = {\n",
    "    'RedNeuronalSimple': 'resultados_simple.csv',\n",
    "    'RedNeuronalIntermedia': 'resultados_intermedia.csv',\n",
    "    'RedNeuronalAvanzada': 'resultados_avanzada.csv'\n",
    "}\n",
    "\n",
    "# Guardar resultados de cada modelo\n",
    "for model_name, model_results in all_results.items():\n",
    "    # Crear lista para almacenar datos del CSV\n",
    "    csv_data = []\n",
    "\n",
    "    print(f\"\\nüìä Procesando {model_name}...\")\n",
    "\n",
    "    # Procesar cada experimento del modelo\n",
    "    for exp_name, results in model_results.items():\n",
    "        params = results['params']\n",
    "\n",
    "        # Crear fila base con informaci√≥n del experimento\n",
    "        base_row = {\n",
    "            'modelo': model_name,\n",
    "            'experimento': exp_name,\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'epochs_total': params['epochs'],\n",
    "            'batch_size': params['batch_size'],\n",
    "            'mejor_epoch': results['best_epoch'],\n",
    "            'mejor_val_accuracy': results['best_val_accuracy']\n",
    "        }\n",
    "\n",
    "        # Agregar m√©tricas por √©poca\n",
    "        for epoch in range(len(results['train_losses'])):\n",
    "            row = base_row.copy()\n",
    "            row.update({\n",
    "                'epoca': epoch + 1,\n",
    "                'train_loss': results['train_losses'][epoch],\n",
    "                'train_accuracy': results['train_accuracies'][epoch],\n",
    "                'val_loss': results['val_losses'][epoch],\n",
    "                'val_accuracy': results['val_accuracies'][epoch],\n",
    "                'es_mejor_modelo': (epoch + 1) == results['best_epoch']\n",
    "            })\n",
    "            csv_data.append(row)\n",
    "\n",
    "    # Convertir a DataFrame y guardar\n",
    "    df_results = pd.DataFrame(csv_data)\n",
    "\n",
    "    # Ordenar por experimento y √©poca para mejor lectura\n",
    "    df_results = df_results.sort_values(['experimento', 'epoca'])\n",
    "\n",
    "    # Nombre del archivo\n",
    "    filename = file_mapping[model_name]\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "\n",
    "    # Guardar CSV\n",
    "    df_results.to_csv(filepath, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"   ‚úÖ {filename} guardado ({len(df_results)} filas)\")\n",
    "    print(f\"      Columnas: {list(df_results.columns)}\")\n",
    "\n",
    "print(f\"\\nüéâ TODOS LOS ARCHIVOS CSV GUARDADOS EN '{results_dir}/'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Mostrar resumen de archivos creados\n",
    "print(\"\\nüìã ARCHIVOS CREADOS:\")\n",
    "for model_name, filename in file_mapping.items():\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        file_size = os.path.getsize(filepath)\n",
    "        print(f\"   üìÑ {filename} ({file_size:,} bytes)\")\n",
    "\n",
    "        # Mostrar preview de las primeras filas\n",
    "        df_preview = pd.read_csv(filepath, nrows=3)\n",
    "        print(f\"      Preview: {len(df_preview)} filas de muestra\")\n",
    "\n",
    "print(f\"\\nüí° Para cargar los resultados posteriormente:\")\n",
    "print(f\"   df_simple = pd.read_csv('{results_dir}/resultados_simple.csv')\")\n",
    "print(f\"   df_intermedia = pd.read_csv('{results_dir}/resultados_intermedia.csv')\")\n",
    "print(f\"   df_avanzada = pd.read_csv('{results_dir}/resultados_avanzada.csv')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de resultados (versi√≥n 1)\n",
    "Se itera sobre `all_results` para mostrar una tabla en consola con:\n",
    "- Modelo\n",
    "- Experimento\n",
    "- Mejor accuracy de validaci√≥n\n",
    "- √âpoca correspondiente\n",
    "- Learning rate y batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà RESUMEN FINAL DE TODOS LOS EXPERIMENTOS\n",
      "================================================================================\n",
      "Modelo               Experimento  Val Accuracy    Mejor √âpoca  LR       Batch   \n",
      "--------------------------------------------------------------------------------\n",
      "RedNeuronalSimple    exp_1        20.03          % 1            0.001    1024    \n",
      "RedNeuronalIntermedia exp_1        19.93          % 1            0.001    1024    \n",
      "RedNeuronalAvanzada  exp_1        20.03          % 1            0.001    1024    \n"
     ]
    }
   ],
   "source": [
    "# Impresi√≥n tabular de los mejores resultados por modelo y experimento.\n",
    "# Se identifica tambi√©n el mejor resultado global (highest val accuracy).\n",
    "\n",
    "# RESUMEN FINAL DE RESULTADOS\n",
    "print(\"\\nüìà RESUMEN FINAL DE TODOS LOS EXPERIMENTOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Cabecera formateada\n",
    "print(f\"{'Modelo':<20} {'Experimento':<12} {'Val Accuracy':<15} {'Mejor √âpoca':<12} {'LR':<8} {'Batch':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "best_overall = {'accuracy': 0, 'model': '', 'exp': '', 'epoch': 0}\n",
    "\n",
    "for model_name, model_results in all_results.items():\n",
    "    for exp_name, results in model_results.items():\n",
    "        accuracy = results['best_val_accuracy']\n",
    "        epoch = results['best_epoch']\n",
    "        lr = results['params']['learning_rate']\n",
    "        batch_size = results['params']['batch_size']\n",
    "\n",
    "        print(f\"{model_name:<20} {exp_name:<12} {accuracy:<15.2f}% {epoch:<12} {lr:<8} {batch_size:<8}\")\n",
    "\n",
    "        if accuracy > best_overall['accuracy']:\n",
    "            best_overall['accuracy'] = accuracy\n",
    "            best_overall['model'] = model_name\n",
    "            best_overall['exp'] = exp_name\n",
    "            best_overall['epoch'] = epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizaci√≥n de curvas de entrenamiento\n",
    "Esta secci√≥n carga archivos CSV con m√©tricas por √©poca y traza:\n",
    "- P√©rdida (train vs valid)\n",
    "- Accuracy (train vs valid)\n",
    "\n",
    "### Funci√≥n\n",
    "`plot_curvas(filepath, titulo, experimento)` filtra por experimento y genera dos figuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidad para visualizar evoluci√≥n de p√©rdida y accuracy a partir de CSVs guardados previamente.\n",
    "# Cada CSV debe contener columnas: epoca, train_loss, val_loss, train_accuracy, val_accuracy, experimento.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_curvas(filepath, titulo, experimento):\n",
    "    # Carga del CSV con resultados hist√≥ricos\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Filtra filas del experimento espec√≠fico\n",
    "    df = df[df[\"experimento\"] == experimento]\n",
    "\n",
    "    # Curva de p√©rdida (entrenamiento vs validaci√≥n)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[\"epoca\"], df[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(df[\"epoca\"], df[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.title(f\"P√©rdida por √âpoca - {titulo} ({experimento})\")\n",
    "    plt.xlabel(\"√âpoca\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Curva de accuracy (entrenamiento vs validaci√≥n)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[\"epoca\"], df[\"train_accuracy\"], label=\"Train Accuracy\")\n",
    "    plt.plot(df[\"epoca\"], df[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "    plt.title(f\"Accuracy por √âpoca - {titulo} ({experimento})\")\n",
    "    plt.xlabel(\"√âpoca\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Llamadas de ejemplo para cada combinaci√≥n modelo/experimento\n",
    "plot_curvas(\"./resultados/resultados_simple.csv\", \"Red Neuronal Simple\", \"exp_1\")\n",
    "plot_curvas(\"./resultados/resultados_simple.csv\", \"Red Neuronal Simple\", \"exp_2\")\n",
    "plot_curvas(\"./resultados/resultados_intermedia.csv\", \"Red Neuronal Intermedia\", \"exp_1\")\n",
    "plot_curvas(\"./resultados/resultados_intermedia.csv\", \"Red Neuronal Intermedia\", \"exp_2\")\n",
    "plot_curvas(\"./resultados/resultados_avanzada.csv\", \"Red Neuronal Avanzada\", \"exp_1\")\n",
    "plot_curvas(\"./resultados/resultados_avanzada.csv\", \"Red Neuronal Avanzada\", \"exp_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versi√≥n 2 del pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segunda versi√≥n del pipeline con dise√±o m√°s modular y m√©tricas ampliadas.\n",
    "# Incluye: clase base, early stopping con paciencia fija, F1 macro/micro y matriz de confusi√≥n.\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================\n",
    "# 1. Modelos con nn.Embedding + pooling\n",
    "# ============================================\n",
    "class BaseMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_layers, num_classes=5, max_length=180):\n",
    "        super(BaseMLP, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding entrenado desde cero\n",
    "        self.max_length = max_length\n",
    "\n",
    "        layers = []\n",
    "        input_dim = embedding_dim\n",
    "        # Construcci√≥n din√°mica de capas ocultas: Linear + ReLU + Dropout\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        # Capa final de clasificaci√≥n\n",
    "        layers.append(nn.Linear(input_dim, num_classes))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)            # [batch, seq_len, embedding_dim]\n",
    "        pooled = emb.mean(dim=1)           # Promedio simple (incluye padding)\n",
    "        return self.network(pooled)        # Logits\n",
    "\n",
    "# Constructores ligeros (envoltorios) para cada variante\n",
    "# Permiten controlar embedding_dim y profundidad desde un solo lugar.\n",
    "def RedNeuronalSimple(vocab_size, embedding_dim=128):\n",
    "    return BaseMLP(vocab_size, embedding_dim, hidden_layers=[128])\n",
    "\n",
    "def RedNeuronalIntermedia(vocab_size, embedding_dim=128):\n",
    "    return BaseMLP(vocab_size, embedding_dim, hidden_layers=[256, 128, 64])\n",
    "\n",
    "def RedNeuronalAvanzada(vocab_size, embedding_dim=128):\n",
    "    return BaseMLP(vocab_size, embedding_dim, hidden_layers=[512, 256, 128, 64])\n",
    "\n",
    "# ============================================\n",
    "# 2. Funciones de entrenamiento y evaluaci√≥n\n",
    "# ============================================\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return running_loss / len(train_loader), 100 * correct / total\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return (\n",
    "        running_loss / len(val_loader),\n",
    "        100 * correct / total,\n",
    "        np.array(all_labels),\n",
    "        np.array(all_preds)\n",
    "    )\n",
    "\n",
    "# ============================================\n",
    "# 3. Entrenamiento con Early Stopping (paciencia fija)\n",
    "# ============================================\n",
    "def train_model_es(model, train_loader, val_loader, optimizer, criterion, epochs, device, patience=25):\n",
    "    best_val_accuracy, best_model_state, best_epoch = 0.0, None, 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        history[\"epoch\"].append(epoch+1)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"üìä √âpoca {epoch+1}/{epochs} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            best_epoch = epoch + 1\n",
    "            no_improve_epochs = 0\n",
    "            print(\"üåü Nuevo mejor modelo!\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"‚èπÔ∏è Early Stopping activado en √©poca {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\nüèÜ Mejor modelo en √©poca {best_epoch} con Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "\n",
    "    return model, best_val_accuracy, best_epoch, pd.DataFrame(history)\n",
    "\n",
    "# ============================================\n",
    "# 4. Evaluaci√≥n final con m√©tricas adicionales\n",
    "# ============================================\n",
    "def evaluar_metricas(model, test_loader, criterion, device, nombre_modelo, exp_name):\n",
    "    model.eval()\n",
    "    _, acc, y_true, y_pred = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "    print(\"\\nüìä Evaluaci√≥n final en Test Set\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1-score Macro:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    print(\"F1-score Micro:\", f1_score(y_true, y_pred, average=\"micro\"))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "    # Matriz de confusi√≥n (visi√≥n de errores entre clases)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Matriz de confusi√≥n - {nombre_modelo} ({exp_name})\")\n",
    "    plt.xlabel(\"Predicho\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 5. Guardado de resultados por √©poca\n",
    "# ============================================\n",
    "results_dir = \"resultados\"\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "resultados_globales = []  # Acumula resumen de mejores resultados\n",
    "\n",
    "def guardar_resultados(nombre_modelo, exp_name, params, best_epoch, best_val_accuracy, history):\n",
    "    filename = f\"{nombre_modelo.lower()}_{exp_name}_idioma.csv\"\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "\n",
    "    # Enriquecer dataframe de historial con metadatos\n",
    "    history[\"modelo\"] = nombre_modelo\n",
    "    history[\"experimento\"] = exp_name\n",
    "    history[\"learning_rate\"] = params[\"learning_rate\"]\n",
    "    history[\"batch_size\"] = params[\"batch_size\"]\n",
    "    history[\"epochs_total\"] = params[\"epochs\"]\n",
    "    history[\"mejor_epoch\"] = best_epoch\n",
    "    history[\"mejor_val_accuracy\"] = best_val_accuracy\n",
    "\n",
    "    history.to_csv(filepath, index=False)\n",
    "    print(f\"‚úÖ Resultados completos guardados en {filepath}\")\n",
    "\n",
    "    resultados_globales.append({\n",
    "        \"modelo\": nombre_modelo,\n",
    "        \"experimento\": exp_name,\n",
    "        \"lr\": params[\"learning_rate\"],\n",
    "        \"batch_size\": params[\"batch_size\"],\n",
    "        \"mejor_epoch\": best_epoch,\n",
    "        \"mejor_val_accuracy\": best_val_accuracy\n",
    "    })\n",
    "\n",
    "# ============================================\n",
    "# 6. Resumen consolidado de mejores resultados\n",
    "# ============================================\n",
    "def mostrar_resumen():\n",
    "    df_resumen = pd.DataFrame(resultados_globales)\n",
    "    print(\"\\nüìà RESUMEN CONSOLIDADO:\")\n",
    "    print(df_resumen)\n",
    "    return df_resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecuci√≥n consolidada de la segunda versi√≥n del pipeline.\n",
    "# Entrena y eval√∫a las tres variantes MLP sobre los splits definidos anteriormente.\n",
    "\n",
    "# ============================================\n",
    "# üöÄ Entrenamiento de los 3 modelos con dataset en ingl√©s\n",
    "# ============================================\n",
    "\n",
    "# Par√°metros generales\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"üîß Dispositivo de entrenamiento: {device}\")\n",
    "\n",
    "# Tama√±o de vocabulario (grande: impacto en memoria y convergencia de embedding)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"üìö Tama√±o del vocabulario: {vocab_size}\")\n",
    "\n",
    "# Modelos disponibles (embedding_dim por defecto = 128)\n",
    "modelos = {\n",
    "    \"RedNeuronalSimple\": lambda: RedNeuronalSimple(vocab_size),\n",
    "    \"RedNeuronalIntermedia\": lambda: RedNeuronalIntermedia(vocab_size),\n",
    "    \"RedNeuronalAvanzada\": lambda: RedNeuronalAvanzada(vocab_size),\n",
    "}\n",
    "\n",
    "# Configuraciones experimentales (epochs exp_2 = 400 aqu√≠ vs 500 en versi√≥n 1)\n",
    "experimentos = {\n",
    "    \"exp_1\": {\"learning_rate\": 0.001, \"epochs\": 100, \"batch_size\": 1024},\n",
    "    \"exp_2\": {\"learning_rate\": 0.0005, \"epochs\": 400, \"batch_size\": 2048},\n",
    "}\n",
    "\n",
    "# Loop principal sobre modelos y configuraciones\n",
    "for nombre_modelo, modelo_fn in modelos.items():\n",
    "    for exp_name, params in experimentos.items():\n",
    "        print(f\"\\nüöÄ Entrenando {nombre_modelo} - {exp_name}\")\n",
    "\n",
    "        # DataLoaders (shuffle solo en train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader   = DataLoader(validation_dataset, batch_size=params[\"batch_size\"])  # no shuffle\n",
    "        test_loader  = DataLoader(test_dataset, batch_size=params[\"batch_size\"])        # evaluaci√≥n\n",
    "\n",
    "        # Instancia y env√≠o a dispositivo\n",
    "        modelo = modelo_fn().to(device)\n",
    "        optimizer = optim.Adam(modelo.parameters(), lr=params[\"learning_rate\"])  # Sin weight decay ni scheduler\n",
    "        criterion = nn.CrossEntropyLoss()                                          # P√©rdida multiclase est√°ndar\n",
    "\n",
    "        # Entrenamiento con early stopping (paciencia fija = 25)\n",
    "        modelo, best_val_acc, best_epoch, history = train_model_es(\n",
    "            modelo, train_loader, val_loader, optimizer, criterion,\n",
    "            epochs=params[\"epochs\"], device=device, patience=25\n",
    "        )\n",
    "\n",
    "        # Guardar historial y metadatos\n",
    "        guardar_resultados(nombre_modelo, exp_name, params, best_epoch, best_val_acc, history)\n",
    "\n",
    "        # Evaluaci√≥n exhaustiva en test (Accuracy + F1 + matriz de confusi√≥n)\n",
    "        evaluar_metricas(modelo, test_loader, criterion, device, nombre_modelo, exp_name)\n",
    "\n",
    "# Mostrar resumen consolidado final\n",
    "df_resumen = mostrar_resumen()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00487558d9014207a51a42c2cabf343a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b519db5dc614d0d887b9caeb23d7af5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ffd206b6b32348d5abd59a9ee00cc9f9",
       "IPY_MODEL_f08087b41add4088be295f80bf402342",
       "IPY_MODEL_337b0494b0174065a0be5ae68f71810b"
      ],
      "layout": "IPY_MODEL_a902b7d10b664f21a2e59926b0271b47"
     }
    },
    "0b9f07efe09547e898f7cd8bdb512ae4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d886fc50d1f4526bdf1dfd6f0effc20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9ca105a16a9416e8eff1c16881cc47e",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a6ca5a11b66f454d96c0f8d137b3ce26",
      "value": "Map:‚Äá100%"
     }
    },
    "102af68cb5e94a11bd1b5bac4ca7b226": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fd2ae367c2c42d097f24c13da029c89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "258b525b6fb4414ca778cfb6f4a78127": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b4c236665374af5a41f6a02442a0433": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a505b074e99f464f8bc386196d83ea87",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_258b525b6fb4414ca778cfb6f4a78127",
      "value": "‚Äá120000/120000‚Äá[00:43&lt;00:00,‚Äá2962.60‚Äáexamples/s]"
     }
    },
    "2cd725340a8146b88465f66a8469002f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f93d3b96972433588145aefea6ace26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32a32309df374ac6a24863006c2c6576": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "337b0494b0174065a0be5ae68f71810b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69879fd52738486cb4b1c698c1657583",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_992ba7781ad34fb2a8f69ccc1b638649",
      "value": "‚Äá972000/972000‚Äá[10:31&lt;00:00,‚Äá2463.18‚Äáexamples/s]"
     }
    },
    "3aa53de1aab44be1be0249c4030d7826": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ba1b5a5ffa5444d9331dffc88092027": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b9f07efe09547e898f7cd8bdb512ae4",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_a7526e7f159b4a4c98bf1cbb8acec48f",
      "value": "Map:‚Äá100%"
     }
    },
    "3ed11a3f7b344580a6f02797b5778531": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "44258844ff6d4e54b4b95f26d6e6ee2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3ba1b5a5ffa5444d9331dffc88092027",
       "IPY_MODEL_d8a0297c325e48c0927284900e7c07a1",
       "IPY_MODEL_2b4c236665374af5a41f6a02442a0433"
      ],
      "layout": "IPY_MODEL_102af68cb5e94a11bd1b5bac4ca7b226"
     }
    },
    "69879fd52738486cb4b1c698c1657583": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b91197db3f14bc7948521a1f231dbc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0d886fc50d1f4526bdf1dfd6f0effc20",
       "IPY_MODEL_e70b0a1e53c14fc786c23d4064269d3a",
       "IPY_MODEL_ffdc4abe356e4f7d802e877e315239a5"
      ],
      "layout": "IPY_MODEL_00487558d9014207a51a42c2cabf343a"
     }
    },
    "7ebecc96e36c42528a79e7b643705889": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "992ba7781ad34fb2a8f69ccc1b638649": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bf92d372bd14d36ac9c2c73057ab848": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a505b074e99f464f8bc386196d83ea87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6ca5a11b66f454d96c0f8d137b3ce26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7526e7f159b4a4c98bf1cbb8acec48f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a902b7d10b664f21a2e59926b0271b47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9ca105a16a9416e8eff1c16881cc47e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8209025d8f54d0493d51a17d4f61243": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d083d45b9d01409fac5e25aecaf708cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8a0297c325e48c0927284900e7c07a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8209025d8f54d0493d51a17d4f61243",
      "max": 120000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9bf92d372bd14d36ac9c2c73057ab848",
      "value": 120000
     }
    },
    "e70b0a1e53c14fc786c23d4064269d3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f93d3b96972433588145aefea6ace26",
      "max": 108000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ed11a3f7b344580a6f02797b5778531",
      "value": 108000
     }
    },
    "f08087b41add4088be295f80bf402342": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d083d45b9d01409fac5e25aecaf708cf",
      "max": 972000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1fd2ae367c2c42d097f24c13da029c89",
      "value": 972000
     }
    },
    "ffd206b6b32348d5abd59a9ee00cc9f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32a32309df374ac6a24863006c2c6576",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_3aa53de1aab44be1be0249c4030d7826",
      "value": "Map:‚Äá100%"
     }
    },
    "ffdc4abe356e4f7d802e877e315239a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ebecc96e36c42528a79e7b643705889",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_2cd725340a8146b88465f66a8469002f",
      "value": "‚Äá108000/108000‚Äá[00:39&lt;00:00,‚Äá2883.66‚Äáexamples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
