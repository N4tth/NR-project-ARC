{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "Este notebook aborda la clasificación del número de estrellas (1–5) asignadas a una reseña de Amazon a partir de su texto. El flujo general incluye: descarga del dataset, construcción del texto de entrada, particionado en conjuntos de entrenamiento/validación/test, tokenización subword usando el tokenizador de XLM-R, definición de varias arquitecturas MLP basadas en un embedding entrenado desde cero, ejecución de experimentos con distintos hiperparámetros, visualización de curvas y evaluación con métricas (accuracy y F1). A lo largo del documento se explican decisiones, supuestos y limitaciones.\n",
    "\n",
    "### Objetivo\n",
    "Predecir la etiqueta (número de estrellas) optimizando la función de pérdida de entropía cruzada y comparar arquitecturas de complejidad creciente.\n",
    "\n",
    "### Guía rápida de secciones\n",
    "1. Carga y exploración del dataset\n",
    "2. Preprocesamiento del texto y creación de etiquetas\n",
    "3. Conversión a `Dataset` de Hugging Face y splits\n",
    "4. Tokenización y preparación de tensores\n",
    "5. Definición de arquitecturas MLP\n",
    "6. Entrenamiento y ejecución de experimentos\n",
    "7. Visualización y métricas adicionales\n",
    "8. Conclusiones y observaciones finales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4MnLgRUkFGC3"
   },
   "outputs": [],
   "source": [
    "# Importaciones principales\n",
    "# En esta celda se cargan las librerías necesarias para:\n",
    "# - Manipulación de datos: pandas\n",
    "# - Descarga del dataset desde Kaggle: kagglehub\n",
    "# - Construcción y entrenamiento de modelos: PyTorch (torch, nn, optim)\n",
    "# - Métricas: accuracy_score, classification_report (sklearn)\n",
    "# - Visualización: matplotlib\n",
    "# - Tokenización y manejo de datasets: datasets (HF) y transformers (AutoTokenizer)\n",
    "\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "from kagglehub import KaggleDatasetAdapter\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y exploración del dataset\n",
    "En esta sección se descarga el dataset de reseñas de Amazon usando `kagglehub` y se carga el archivo `train.csv` en un DataFrame de pandas.\n",
    "\n",
    "### Objetivos\n",
    "- Verificar forma (número de filas/columnas).\n",
    "- Inspeccionar nombres de columnas disponibles.\n",
    "- Observar primeras filas para entender estructura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar archivo\n",
    "file_path = kagglehub.dataset_download(\"mexwell/amazon-reviews-multi\")\n",
    "\n",
    "csv_path = file_path + \"/train.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path, encoding=\"latin-1\")\n",
    "\n",
    "print(\"Shape:\", df.shape)\n",
    "print(\"Columnas:\", df.columns)\n",
    "print(\"Primeras 5 filas:\\n\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de texto y creación de etiquetas\n",
    "Se unifica el título y el cuerpo de la reseña en un único campo `text` para maximizar el contexto disponible. Además, se ajustan las etiquetas de estrellas para que comiencen en 0 (requisito de `CrossEntropyLoss`).\n",
    "\n",
    "### Detalles\n",
    "- `text = review_title + ' ' + review_body` con `fillna('')` para evitar valores nulos.\n",
    "- `labels = stars - 1` transforma el rango [1,5] a [0,4].\n",
    "- Se conservan solo columnas relevantes: `text`, `labels`, `language`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unificación de título y cuerpo de la reseña + creación de etiquetas normalizadas\n",
    "# Se genera un único campo de texto para aprovechar todo el contexto disponible.\n",
    "# Las etiquetas (número de estrellas) se desplazan a rango 0..4, requerido por CrossEntropyLoss.\n",
    "# Se retienen únicamente columnas necesarias para el modelo.\n",
    "# Nota: No se hace limpieza adicional (lowercase, eliminar signos, etc.) porque el tokenizador subword maneja vocab enriquecido.\n",
    "\n",
    "# Combina título y cuerpo (maneja NaN con cadenas vacías)\n",
    "df[\"text\"] = df[\"review_title\"].fillna(\"\") + \" \" + df[\"review_body\"].fillna(\"\")\n",
    "# Ajusta etiquetas al rango 0-4\n",
    "df[\"labels\"] = df[\"stars\"] - 1\n",
    "# Subconjunto de columnas relevantes\n",
    "df = df[[\"text\", \"labels\", \"language\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversión a Dataset y particionado (train/validation/test)\n",
    "Se convierte el DataFrame a un `datasets.Dataset` de Hugging Face y se generan tres subconjuntos:\n",
    "- `train_dataset`: datos para entrenamiento.\n",
    "- `validation_dataset`: para monitorear generalización y aplicar early stopping.\n",
    "- `test_dataset`: evaluación final.\n",
    "\n",
    "### Detalles\n",
    "- Primer split: 90% train + 10% test.\n",
    "- Segundo split: del train original se separa 10% para validación (≈ 9% total).\n",
    "- Semilla fija `seed=42` para reproducibilidad del particionado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión del DataFrame a un objeto Dataset de Hugging Face\n",
    "# Esto facilita operaciones vectorizadas, mapeo y compatibilidad con DataLoader posteriormente.\n",
    "# Se crean dos niveles de splits: train/test y luego train/validation.\n",
    "\n",
    "#Conversión a hugging face dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# División del dataset en train y test (10% test)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "# División adicional para generar validación desde el train restante\n",
    "train_val = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "# Asignación final de subconjuntos\n",
    "train_dataset = train_val[\"train\"]\n",
    "validation_dataset = train_val[\"test\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenización y preparación de tensores\n",
    "Se utiliza el tokenizador subword de `xlm-roberta-base` para segmentar texto multilingüe.\n",
    "\n",
    "### Parámetros clave\n",
    "- `truncation=True`: corta reseñas largas para no exceder `max_length`.\n",
    "- `padding=\"max_length\"`: fija longitud uniforme (facilita batching, pero introduce tokens PAD en reseñas cortas).\n",
    "- `max_length=180`: compromiso entre cobertura contextual y coste computacional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenización subword con el tokenizador de XLM-R\n",
    "model_name = \"xlm-roberta-base\"\n",
    "# Carga del tokenizador multilingüe\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Función de preprocesamiento (mapeable sobre batches)\n",
    "def preprocess(batch):\n",
    "  return tokenizer(\n",
    "      batch[\"text\"],               # Lista de textos\n",
    "      truncation=True,              # Corta secuencias largas\n",
    "      padding=\"max_length\",        # Padding uniforme hasta max_length\n",
    "      max_length=180                # Longitud máxima (hiperparámetro manual)\n",
    "  )\n",
    "\n",
    "# Aplicación vectorizada a cada subconjunto\n",
    "train_dataset = train_dataset.map(preprocess, batched=True)\n",
    "validation_dataset = validation_dataset.map(preprocess, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Selección de columnas y conversión a tensores de PyTorch\n",
    "train_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "validation_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "test_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de arquitecturas MLP basadas en embeddings\n",
    "Se definen tres variantes de redes neuronales densas (MLP) que comparten:\n",
    "- Capa de embeddings entrenada desde cero (dimensión configurable).\n",
    "- Pooling por promedio sobre la secuencia.\n",
    "- Capas densas intermedias con activación ReLU y Dropout (0.2).\n",
    "- Capa final lineal a 5 clases (sin softmax explícito porque `CrossEntropyLoss` lo combina internamente).\n",
    "\n",
    "### Diferencias principales\n",
    "- Simple: Menor profundidad y capacidad.\n",
    "- Intermedia: Tres capas ocultas progresivas.\n",
    "- Avanzada: Cuatro capas ocultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de tres arquitecturas MLP de distinta profundidad.\n",
    "# Patrón común:\n",
    "# 1. Embedding: convierte IDs de subwords en vectores densos entrenables.\n",
    "# 2. Pooling (mean): colapsa dimensión secuencial -> vector fijo.\n",
    "# 3. Capas densas con ReLU + Dropout (0.2) para introducir no linealidad y regularización.\n",
    "# 4. Capa de salida: logits para 5 clases (estrellas 0..4).\n",
    "# Nota: se usa CrossEntropyLoss posteriormente, por eso no hay Softmax aquí.\n",
    "\n",
    "class RedNeuronalSimple(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim1=128, hidden_dim2=64, output_dim=5, pad_idx=0):\n",
    "        super(RedNeuronalSimple, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Capa oculta 1\n",
    "        self.capa_oculta_1 = nn.Linear(embedding_dim, hidden_dim1)\n",
    "        self.activacion_1 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa oculta 2\n",
    "        self.capa_oculta_2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.activacion_2 = nn.ReLU()\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.capa_salida = nn.Linear(hidden_dim2, output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)        \n",
    "        pooled = embedded.mean(dim=1)             \n",
    "        x = self.capa_oculta_1(pooled)\n",
    "        x = self.activacion_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.capa_oculta_2(x)\n",
    "        x = self.activacion_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.capa_salida(x)                   \n",
    "        return x\n",
    "\n",
    "class RedNeuronalIntermedia(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim1=256, hidden_dim2=128, hidden_dim3=64, output_dim=5, pad_idx=0):\n",
    "        super(RedNeuronalIntermedia, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # Capa oculta 1\n",
    "        self.capa_oculta_1 = nn.Linear(embedding_dim, hidden_dim1)\n",
    "        self.activacion_1 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa oculta 2\n",
    "        self.capa_oculta_2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.activacion_2 = nn.ReLU()\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa oculta 3\n",
    "        self.capa_oculta_3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.activacion_3 = nn.ReLU()\n",
    "        self.dropout_3 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.capa_salida = nn.Linear(hidden_dim3, output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        pooled = embedded.mean(dim=1)\n",
    "        x = self.capa_oculta_1(pooled)\n",
    "        x = self.activacion_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.capa_oculta_2(x)\n",
    "        x = self.activacion_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.capa_oculta_3(x)\n",
    "        x = self.activacion_3(x)\n",
    "        x = self.dropout_3(x)\n",
    "        x = self.capa_salida(x)\n",
    "        return x\n",
    "\n",
    "class RedNeuronalAvanzada(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim1=512, hidden_dim2=256, hidden_dim3=128, hidden_dim4=64, output_dim=5, pad_idx=0):\n",
    "        super(RedNeuronalAvanzada, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        # Capa oculta 1\n",
    "        self.capa_oculta_1 = nn.Linear(embedding_dim, hidden_dim1)\n",
    "        self.activacion_1 = nn.ReLU()\n",
    "        self.dropout_1 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa oculta 2\n",
    "        self.capa_oculta_2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.activacion_2 = nn.ReLU()\n",
    "        self.dropout_2 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa oculta 3\n",
    "        self.capa_oculta_3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
    "        self.activacion_3 = nn.ReLU()\n",
    "        self.dropout_3 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa oculta 4\n",
    "        self.capa_oculta_4 = nn.Linear(hidden_dim3, hidden_dim4)\n",
    "        self.activacion_4 = nn.ReLU()\n",
    "        self.dropout_4 = nn.Dropout(0.2)\n",
    "\n",
    "        # Capa de salida\n",
    "        self.capa_salida = nn.Linear(hidden_dim4, output_dim)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids)\n",
    "        pooled = embedded.mean(dim=1)\n",
    "        x = self.capa_oculta_1(pooled)\n",
    "        x = self.activacion_1(x)\n",
    "        x = self.dropout_1(x)\n",
    "        x = self.capa_oculta_2(x)\n",
    "        x = self.activacion_2(x)\n",
    "        x = self.dropout_2(x)\n",
    "        x = self.capa_oculta_3(x)\n",
    "        x = self.activacion_3(x)\n",
    "        x = self.dropout_3(x)\n",
    "        x = self.capa_oculta_4(x)\n",
    "        x = self.activacion_4(x)\n",
    "        x = self.dropout_4(x)\n",
    "        x = self.capa_salida(x)\n",
    "        return x\n",
    "\n",
    "def setup_training(model, learning_rate=0.001):\n",
    "    # Configura Adam como optimizador y CrossEntropy para clasificación multiclase.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    return optimizer, criterion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de entrenamiento y evaluación (versión 1)\n",
    "Se definen helpers para entrenar por época, evaluar en validación y un loop principal con early stopping basado en *validation accuracy*.\n",
    "\n",
    "### Componentes\n",
    "- `train_epoch`: calcula pérdida media y accuracy en entrenamiento.\n",
    "- `evaluate_model`: calcula pérdida y accuracy en validación sin gradientes.\n",
    "- `train_model`: orquesta múltiples épocas, aplica early stopping proporcional al número total de épocas, y guarda el mejor estado del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Dispositivo de entrenamiento: cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuración del dispositivo (GPU si disponible, de lo contrario CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🔧 Dispositivo de entrenamiento: {device}\")\n",
    "\n",
    "# Función de entrenamiento por época\n",
    "# Recorre todos los batches, acumula pérdida y calcula accuracy acumulado.\n",
    "# Nota: El embedding se entrena desde cero; grandes batches pueden suavizar gradientes.\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    \"\"\"Entrena el modelo por una época y devuelve (loss_media, accuracy).\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Datos del batch (input_ids y labels ya en tensores Torch)\n",
    "        inputs = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()              # Limpia gradientes previos\n",
    "        outputs = model(inputs)            # Forward: logits [batch, num_clases]\n",
    "        loss = criterion(outputs, labels)  # CrossEntropyLoss (logits + labels enteros)\n",
    "\n",
    "        loss.backward()                    # Backpropagation\n",
    "        optimizer.step()                   # Actualiza parámetros\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)   # Clase más probable\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "# Función de evaluación (sin actualización de gradientes)\n",
    "# Calcula pérdida y accuracy en validación para monitorear generalización.\n",
    "def evaluate_model(model, val_loader, criterion, device):\n",
    "    \"\"\"Evalúa el modelo en el conjunto de validación y devuelve (loss_media, accuracy).\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    return val_loss, val_accuracy\n",
    "\n",
    "# Loop principal de entrenamiento con early stopping basado en validation accuracy.\n",
    "# Guarda el mejor estado (mayor val_accuracy) y detiene si no mejora tras 'patience' épocas.\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, epochs, device, model_name, exp_name):\n",
    "    \"\"\"Entrenamiento multiepoch con early stopping. Devuelve métricas y mejor modelo.\"\"\"\n",
    "    print(f\"\\n🚀 Iniciando entrenamiento - {model_name} - {exp_name}\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    train_losses, train_accuracies = [], []\n",
    "    val_losses, val_accuracies = [], []\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    best_model_state = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    patience = max(1, epochs // 4)   # Paciencia proporcional al número máximo de épocas\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n📊 Época {epoch + 1}/{epochs}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"⏳ Sin mejora en val_acc. Paciencia: {patience_counter}/{patience}\")\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"⏹️ Early Stopping activado en época {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val   Loss: {val_loss:.4f} | Val   Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "\n",
    "    print(f\"\\n✅ Entrenamiento completado!\")\n",
    "    print(f\"🏆 Mejor modelo: Época {best_epoch} con Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "\n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_losses': val_losses,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'best_val_accuracy': best_val_accuracy,\n",
    "        'best_epoch': best_epoch,\n",
    "        'stopped_epoch': epoch +1,\n",
    "        'model': model\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecución de experimentos (versión 1)\n",
    "Se itera sobre las tres arquitecturas definidas y dos configuraciones de hiperparámetros.\n",
    "\n",
    "### Experimentos\n",
    "- `exp_1`: lr=0.001, epochs=100, batch=1024\n",
    "- `exp_2`: lr=0.0005, epochs=500, batch=2048\n",
    "\n",
    "### Objetivo\n",
    "Comparar el impacto de profundidad y número de épocas / tamaño de batch sobre la accuracy de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 INICIANDO EXPERIMENTOS DE REDES NEURONALES\n",
      "================================================================================\n",
      "\n",
      "🔬 MODELO: RedNeuronalSimple\n",
      "============================================================\n",
      "\n",
      "📋 Experimento: exp_1\n",
      "   Parámetros: {'learning_rate': 0.001, 'epochs': 1, 'batch_size': 1024}\n",
      "\n",
      "🚀 Iniciando entrenamiento - RedNeuronalSimple - exp_1\n",
      "======================================================================\n",
      "\n",
      "📊 Época 1/1\n",
      "--------------------------------------------------\n",
      "Train Loss: 63.8235 | Train Acc: 20.01%\n",
      "Val Loss: 1.6094   | Val Acc: 20.03%\n",
      "🌟 ¡Nuevo mejor modelo!\n",
      "\n",
      "✅ Entrenamiento completado!\n",
      "🏆 Mejor modelo: Época 1 con Val Accuracy: 20.03%\n",
      "\n",
      "🔬 MODELO: RedNeuronalIntermedia\n",
      "============================================================\n",
      "\n",
      "📋 Experimento: exp_1\n",
      "   Parámetros: {'learning_rate': 0.001, 'epochs': 1, 'batch_size': 1024}\n",
      "\n",
      "🚀 Iniciando entrenamiento - RedNeuronalIntermedia - exp_1\n",
      "======================================================================\n",
      "\n",
      "📊 Época 1/1\n",
      "--------------------------------------------------\n",
      "Train Loss: 63.8235 | Train Acc: 20.01%\n",
      "Val Loss: 1.6094   | Val Acc: 20.03%\n",
      "🌟 ¡Nuevo mejor modelo!\n",
      "\n",
      "✅ Entrenamiento completado!\n",
      "🏆 Mejor modelo: Época 1 con Val Accuracy: 20.03%\n",
      "\n",
      "🔬 MODELO: RedNeuronalIntermedia\n",
      "============================================================\n",
      "\n",
      "📋 Experimento: exp_1\n",
      "   Parámetros: {'learning_rate': 0.001, 'epochs': 1, 'batch_size': 1024}\n",
      "\n",
      "🚀 Iniciando entrenamiento - RedNeuronalIntermedia - exp_1\n",
      "======================================================================\n",
      "\n",
      "📊 Época 1/1\n",
      "--------------------------------------------------\n",
      "Train Loss: 23.4727 | Train Acc: 20.07%\n",
      "Val Loss: 1.6095   | Val Acc: 19.93%\n",
      "🌟 ¡Nuevo mejor modelo!\n",
      "\n",
      "✅ Entrenamiento completado!\n",
      "🏆 Mejor modelo: Época 1 con Val Accuracy: 19.93%\n",
      "\n",
      "🔬 MODELO: RedNeuronalAvanzada\n",
      "============================================================\n",
      "\n",
      "📋 Experimento: exp_1\n",
      "   Parámetros: {'learning_rate': 0.001, 'epochs': 1, 'batch_size': 1024}\n",
      "\n",
      "🚀 Iniciando entrenamiento - RedNeuronalAvanzada - exp_1\n",
      "======================================================================\n",
      "\n",
      "📊 Época 1/1\n",
      "--------------------------------------------------\n",
      "Train Loss: 23.4727 | Train Acc: 20.07%\n",
      "Val Loss: 1.6095   | Val Acc: 19.93%\n",
      "🌟 ¡Nuevo mejor modelo!\n",
      "\n",
      "✅ Entrenamiento completado!\n",
      "🏆 Mejor modelo: Época 1 con Val Accuracy: 19.93%\n",
      "\n",
      "🔬 MODELO: RedNeuronalAvanzada\n",
      "============================================================\n",
      "\n",
      "📋 Experimento: exp_1\n",
      "   Parámetros: {'learning_rate': 0.001, 'epochs': 1, 'batch_size': 1024}\n",
      "\n",
      "🚀 Iniciando entrenamiento - RedNeuronalAvanzada - exp_1\n",
      "======================================================================\n",
      "\n",
      "📊 Época 1/1\n",
      "--------------------------------------------------\n",
      "Train Loss: 11.5856 | Train Acc: 19.97%\n",
      "Val Loss: 1.6094   | Val Acc: 20.03%\n",
      "🌟 ¡Nuevo mejor modelo!\n",
      "\n",
      "✅ Entrenamiento completado!\n",
      "🏆 Mejor modelo: Época 1 con Val Accuracy: 20.03%\n",
      "\n",
      "🎉 TODOS LOS EXPERIMENTOS COMPLETADOS\n",
      "================================================================================\n",
      "Train Loss: 11.5856 | Train Acc: 19.97%\n",
      "Val Loss: 1.6094   | Val Acc: 20.03%\n",
      "🌟 ¡Nuevo mejor modelo!\n",
      "\n",
      "✅ Entrenamiento completado!\n",
      "🏆 Mejor modelo: Época 1 con Val Accuracy: 20.03%\n",
      "\n",
      "🎉 TODOS LOS EXPERIMENTOS COMPLETADOS\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Bucle maestro de experimentos sobre tres arquitecturas y dos configuraciones de hiperparámetros.\n",
    "# Nota: Este bloque se replica más adelante con ligera variación; mantener solo una versión en producción facilitaría mantenimiento.\n",
    "\n",
    "# EJECUCIÓN DE TODOS LOS EXPERIMENTOS\n",
    "print(\"🎯 INICIANDO EXPERIMENTOS DE REDES NEURONALES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Diccionario de clases de modelos\n",
    "models_dict = {\n",
    "    'RedNeuronalSimple': RedNeuronalSimple,\n",
    "    'RedNeuronalIntermedia': RedNeuronalIntermedia,\n",
    "    'RedNeuronalAvanzada': RedNeuronalAvanzada\n",
    "}\n",
    "\n",
    "# Hiperparámetros por experimento\n",
    "experiments = {\n",
    "    \"exp_1\": {\"learning_rate\": 0.001, \"epochs\": 100, \"batch_size\": 1024},\n",
    "    \"exp_2\": {\"learning_rate\": 0.0005, \"epochs\": 500, \"batch_size\": 2048},\n",
    "}\n",
    "\n",
    "# Tamaño del vocabulario del tokenizador (embedding entrenado desde cero)\n",
    "vocab_size = len(tokenizer.vocab)\n",
    "embedding_dim = 180              # Dimensión de embedding arbitraria elegida\n",
    "output_dim = 5                   # Número de clases (estrellas 0..4)\n",
    "pad_idx = tokenizer.pad_token_id # Índice de padding\n",
    "\n",
    "all_results = {}                 # Almacenará métricas y mejores épocas por modelo/experimento\n",
    "\n",
    "for model_name, ModelClass in models_dict.items():\n",
    "    print(f\"\\n🔬 MODELO: {model_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    model_results = {}\n",
    "\n",
    "    for exp_name, params in experiments.items():\n",
    "        print(f\"\\n📋 Experimento: {exp_name}\")\n",
    "        print(f\"   Parámetros: {params}\")\n",
    "\n",
    "        # DataLoaders con batch_size específico\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                                 batch_size=params[\"batch_size\"],\n",
    "                                 shuffle=True)\n",
    "        val_loader = DataLoader(validation_dataset,\n",
    "                               batch_size=params[\"batch_size\"],\n",
    "                               shuffle=False)\n",
    "\n",
    "        # Instanciación (red simple pasa hidden_dim explícito)\n",
    "        if model_name == \"RedNeuronalSimple\":\n",
    "            model = ModelClass(vocab_size, embedding_dim, output_dim=output_dim, pad_idx=pad_idx).to(device)\n",
    "        elif model_name == \"RedNeuronalIntermedia\":\n",
    "            model = ModelClass(vocab_size, embedding_dim, output_dim=output_dim, pad_idx=pad_idx).to(device)\n",
    "        elif model_name == \"RedNeuronalAvanzada\":\n",
    "            model = ModelClass(vocab_size, embedding_dim, output_dim=output_dim, pad_idx=pad_idx).to(device)\n",
    "\n",
    "        optimizer, criterion = setup_training(model, learning_rate=params[\"learning_rate\"])\n",
    "\n",
    "        # Entrenamiento y captura de resultados\n",
    "        results = train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            optimizer=optimizer,\n",
    "            criterion=criterion,\n",
    "            epochs=params[\"epochs\"],\n",
    "            device=device,\n",
    "            model_name=model_name,\n",
    "            exp_name=exp_name\n",
    "        )\n",
    "\n",
    "        results['params'] = params\n",
    "        model_results[exp_name] = results\n",
    "\n",
    "    all_results[model_name] = model_results\n",
    "\n",
    "print(f\"\\n🎉 TODOS LOS EXPERIMENTOS COMPLETADOS\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almacenamiento de resultados (versión 1)\n",
    "Se almacenan los resultados en archivos csv para cada modelo y los experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUARDAR RESULTADOS EN ARCHIVOS CSV\n",
    "print(\"\\n💾 GUARDANDO RESULTADOS EN ARCHIVOS CSV\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "import os\n",
    "\n",
    "# Crear directorio de resultados si no existe\n",
    "results_dir = \"resultados\"\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    print(f\"📁 Directorio '{results_dir}' creado\")\n",
    "\n",
    "# Mapeo de nombres de modelos a nombres de archivos\n",
    "file_mapping = {\n",
    "    'RedNeuronalSimple': 'resultados_simple.csv',\n",
    "    'RedNeuronalIntermedia': 'resultados_intermedia.csv',\n",
    "    'RedNeuronalAvanzada': 'resultados_avanzada.csv'\n",
    "}\n",
    "\n",
    "# Guardar resultados de cada modelo\n",
    "for model_name, model_results in all_results.items():\n",
    "    # Crear lista para almacenar datos del CSV\n",
    "    csv_data = []\n",
    "\n",
    "    print(f\"\\n📊 Procesando {model_name}...\")\n",
    "\n",
    "    # Procesar cada experimento del modelo\n",
    "    for exp_name, results in model_results.items():\n",
    "        params = results['params']\n",
    "\n",
    "        # Crear fila base con información del experimento\n",
    "        base_row = {\n",
    "            'modelo': model_name,\n",
    "            'experimento': exp_name,\n",
    "            'learning_rate': params['learning_rate'],\n",
    "            'epochs_total': params['epochs'],\n",
    "            'batch_size': params['batch_size'],\n",
    "            'mejor_epoch': results['best_epoch'],\n",
    "            'mejor_val_accuracy': results['best_val_accuracy']\n",
    "        }\n",
    "\n",
    "        # Agregar métricas por época\n",
    "        for epoch in range(len(results['train_losses'])):\n",
    "            row = base_row.copy()\n",
    "            row.update({\n",
    "                'epoca': epoch + 1,\n",
    "                'train_loss': results['train_losses'][epoch],\n",
    "                'train_accuracy': results['train_accuracies'][epoch],\n",
    "                'val_loss': results['val_losses'][epoch],\n",
    "                'val_accuracy': results['val_accuracies'][epoch],\n",
    "                'es_mejor_modelo': (epoch + 1) == results['best_epoch']\n",
    "            })\n",
    "            csv_data.append(row)\n",
    "\n",
    "    # Convertir a DataFrame y guardar\n",
    "    df_results = pd.DataFrame(csv_data)\n",
    "\n",
    "    # Ordenar por experimento y época para mejor lectura\n",
    "    df_results = df_results.sort_values(['experimento', 'epoca'])\n",
    "\n",
    "    # Nombre del archivo\n",
    "    filename = file_mapping[model_name]\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "\n",
    "    # Guardar CSV\n",
    "    df_results.to_csv(filepath, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"   ✅ {filename} guardado ({len(df_results)} filas)\")\n",
    "    print(f\"      Columnas: {list(df_results.columns)}\")\n",
    "\n",
    "print(f\"\\n🎉 TODOS LOS ARCHIVOS CSV GUARDADOS EN '{results_dir}/'\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Mostrar resumen de archivos creados\n",
    "print(\"\\n📋 ARCHIVOS CREADOS:\")\n",
    "for model_name, filename in file_mapping.items():\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        file_size = os.path.getsize(filepath)\n",
    "        print(f\"   📄 {filename} ({file_size:,} bytes)\")\n",
    "\n",
    "        # Mostrar preview de las primeras filas\n",
    "        df_preview = pd.read_csv(filepath, nrows=3)\n",
    "        print(f\"      Preview: {len(df_preview)} filas de muestra\")\n",
    "\n",
    "print(f\"\\n💡 Para cargar los resultados posteriormente:\")\n",
    "print(f\"   df_simple = pd.read_csv('{results_dir}/resultados_simple.csv')\")\n",
    "print(f\"   df_intermedia = pd.read_csv('{results_dir}/resultados_intermedia.csv')\")\n",
    "print(f\"   df_avanzada = pd.read_csv('{results_dir}/resultados_avanzada.csv')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen de resultados (versión 1)\n",
    "Se itera sobre `all_results` para mostrar una tabla en consola con:\n",
    "- Modelo\n",
    "- Experimento\n",
    "- Mejor accuracy de validación\n",
    "- Época correspondiente\n",
    "- Learning rate y batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 RESUMEN FINAL DE TODOS LOS EXPERIMENTOS\n",
      "================================================================================\n",
      "Modelo               Experimento  Val Accuracy    Mejor Época  LR       Batch   \n",
      "--------------------------------------------------------------------------------\n",
      "RedNeuronalSimple    exp_1        20.03          % 1            0.001    1024    \n",
      "RedNeuronalIntermedia exp_1        19.93          % 1            0.001    1024    \n",
      "RedNeuronalAvanzada  exp_1        20.03          % 1            0.001    1024    \n"
     ]
    }
   ],
   "source": [
    "# Impresión tabular de los mejores resultados por modelo y experimento.\n",
    "# Se identifica también el mejor resultado global (highest val accuracy).\n",
    "\n",
    "# RESUMEN FINAL DE RESULTADOS\n",
    "print(\"\\n📈 RESUMEN FINAL DE TODOS LOS EXPERIMENTOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Cabecera formateada\n",
    "print(f\"{'Modelo':<20} {'Experimento':<12} {'Val Accuracy':<15} {'Mejor Época':<12} {'LR':<8} {'Batch':<8}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "best_overall = {'accuracy': 0, 'model': '', 'exp': '', 'epoch': 0}\n",
    "\n",
    "for model_name, model_results in all_results.items():\n",
    "    for exp_name, results in model_results.items():\n",
    "        accuracy = results['best_val_accuracy']\n",
    "        epoch = results['best_epoch']\n",
    "        lr = results['params']['learning_rate']\n",
    "        batch_size = results['params']['batch_size']\n",
    "\n",
    "        print(f\"{model_name:<20} {exp_name:<12} {accuracy:<15.2f}% {epoch:<12} {lr:<8} {batch_size:<8}\")\n",
    "\n",
    "        if accuracy > best_overall['accuracy']:\n",
    "            best_overall['accuracy'] = accuracy\n",
    "            best_overall['model'] = model_name\n",
    "            best_overall['exp'] = exp_name\n",
    "            best_overall['epoch'] = epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de curvas de entrenamiento\n",
    "Esta sección carga archivos CSV con métricas por época y traza:\n",
    "- Pérdida (train vs valid)\n",
    "- Accuracy (train vs valid)\n",
    "\n",
    "### Función\n",
    "`plot_curvas(filepath, titulo, experimento)` filtra por experimento y genera dos figuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilidad para visualizar evolución de pérdida y accuracy a partir de CSVs guardados previamente.\n",
    "# Cada CSV debe contener columnas: epoca, train_loss, val_loss, train_accuracy, val_accuracy, experimento.\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_curvas(filepath, titulo, experimento):\n",
    "    # Carga del CSV con resultados históricos\n",
    "    df = pd.read_csv(filepath)\n",
    "    # Filtra filas del experimento específico\n",
    "    df = df[df[\"experimento\"] == experimento]\n",
    "\n",
    "    # Curva de pérdida (entrenamiento vs validación)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[\"epoca\"], df[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(df[\"epoca\"], df[\"val_loss\"], label=\"Val Loss\")\n",
    "    plt.title(f\"Pérdida por Época - {titulo} ({experimento})\")\n",
    "    plt.xlabel(\"Época\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Curva de accuracy (entrenamiento vs validación)\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[\"epoca\"], df[\"train_accuracy\"], label=\"Train Accuracy\")\n",
    "    plt.plot(df[\"epoca\"], df[\"val_accuracy\"], label=\"Val Accuracy\")\n",
    "    plt.title(f\"Accuracy por Época - {titulo} ({experimento})\")\n",
    "    plt.xlabel(\"Época\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Llamadas de ejemplo para cada combinación modelo/experimento\n",
    "plot_curvas(\"./resultados/resultados_simple.csv\", \"Red Neuronal Simple\", \"exp_1\")\n",
    "plot_curvas(\"./resultados/resultados_simple.csv\", \"Red Neuronal Simple\", \"exp_2\")\n",
    "plot_curvas(\"./resultados/resultados_intermedia.csv\", \"Red Neuronal Intermedia\", \"exp_1\")\n",
    "plot_curvas(\"./resultados/resultados_intermedia.csv\", \"Red Neuronal Intermedia\", \"exp_2\")\n",
    "plot_curvas(\"./resultados/resultados_avanzada.csv\", \"Red Neuronal Avanzada\", \"exp_1\")\n",
    "plot_curvas(\"./resultados/resultados_avanzada.csv\", \"Red Neuronal Avanzada\", \"exp_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versión 2 del pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segunda versión del pipeline con diseño más modular y métricas ampliadas.\n",
    "# Incluye: clase base, early stopping con paciencia fija, F1 macro/micro y matriz de confusión.\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================\n",
    "# 1. Modelos con nn.Embedding + pooling\n",
    "# ============================================\n",
    "class BaseMLP(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_layers, num_classes=5, max_length=180):\n",
    "        super(BaseMLP, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)  # Embedding entrenado desde cero\n",
    "        self.max_length = max_length\n",
    "\n",
    "        layers = []\n",
    "        input_dim = embedding_dim\n",
    "        # Construcción dinámica de capas ocultas: Linear + ReLU + Dropout\n",
    "        for hidden_dim in hidden_layers:\n",
    "            layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            input_dim = hidden_dim\n",
    "\n",
    "        # Capa final de clasificación\n",
    "        layers.append(nn.Linear(input_dim, num_classes))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb = self.embedding(x)            # [batch, seq_len, embedding_dim]\n",
    "        pooled = emb.mean(dim=1)           # Promedio simple (incluye padding)\n",
    "        return self.network(pooled)        # Logits\n",
    "\n",
    "# Constructores ligeros (envoltorios) para cada variante\n",
    "# Permiten controlar embedding_dim y profundidad desde un solo lugar.\n",
    "def RedNeuronalSimple(vocab_size, embedding_dim=128):\n",
    "    return BaseMLP(vocab_size, embedding_dim, hidden_layers=[128])\n",
    "\n",
    "def RedNeuronalIntermedia(vocab_size, embedding_dim=128):\n",
    "    return BaseMLP(vocab_size, embedding_dim, hidden_layers=[256, 128, 64])\n",
    "\n",
    "def RedNeuronalAvanzada(vocab_size, embedding_dim=128):\n",
    "    return BaseMLP(vocab_size, embedding_dim, hidden_layers=[512, 256, 128, 64])\n",
    "\n",
    "# ============================================\n",
    "# 2. Funciones de entrenamiento y evaluación\n",
    "# ============================================\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        inputs = batch['input_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    return running_loss / len(train_loader), 100 * correct / total\n",
    "\n",
    "\n",
    "def evaluate_model(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "    all_labels, all_preds = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    return (\n",
    "        running_loss / len(val_loader),\n",
    "        100 * correct / total,\n",
    "        np.array(all_labels),\n",
    "        np.array(all_preds)\n",
    "    )\n",
    "\n",
    "# ============================================\n",
    "# 3. Entrenamiento con Early Stopping (paciencia fija)\n",
    "# ============================================\n",
    "def train_model_es(model, train_loader, val_loader, optimizer, criterion, epochs, device, patience=25):\n",
    "    best_val_accuracy, best_model_state, best_epoch = 0.0, None, 0\n",
    "    no_improve_epochs = 0\n",
    "\n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        history[\"epoch\"].append(epoch+1)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"📊 Época {epoch+1}/{epochs} | Train Acc: {train_acc:.2f}% | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        if val_acc > best_val_accuracy:\n",
    "            best_val_accuracy = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            best_epoch = epoch + 1\n",
    "            no_improve_epochs = 0\n",
    "            print(\"🌟 Nuevo mejor modelo!\")\n",
    "        else:\n",
    "            no_improve_epochs += 1\n",
    "\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(f\"⏹️ Early Stopping activado en época {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\n🏆 Mejor modelo en época {best_epoch} con Val Accuracy: {best_val_accuracy:.2f}%\")\n",
    "\n",
    "    return model, best_val_accuracy, best_epoch, pd.DataFrame(history)\n",
    "\n",
    "# ============================================\n",
    "# 4. Evaluación final con métricas adicionales\n",
    "# ============================================\n",
    "def evaluar_metricas(model, test_loader, criterion, device, nombre_modelo, exp_name):\n",
    "    model.eval()\n",
    "    _, acc, y_true, y_pred = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "    print(\"\\n📊 Evaluación final en Test Set\")\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"F1-score Macro:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "    print(\"F1-score Micro:\", f1_score(y_true, y_pred, average=\"micro\"))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "    # Matriz de confusión (visión de errores entre clases)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Matriz de confusión - {nombre_modelo} ({exp_name})\")\n",
    "    plt.xlabel(\"Predicho\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 5. Guardado de resultados por época\n",
    "# ============================================\n",
    "results_dir = \"resultados\"\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "resultados_globales = []  # Acumula resumen de mejores resultados\n",
    "\n",
    "def guardar_resultados(nombre_modelo, exp_name, params, best_epoch, best_val_accuracy, history):\n",
    "    filename = f\"{nombre_modelo.lower()}_{exp_name}_idioma.csv\"\n",
    "    filepath = os.path.join(results_dir, filename)\n",
    "\n",
    "    # Enriquecer dataframe de historial con metadatos\n",
    "    history[\"modelo\"] = nombre_modelo\n",
    "    history[\"experimento\"] = exp_name\n",
    "    history[\"learning_rate\"] = params[\"learning_rate\"]\n",
    "    history[\"batch_size\"] = params[\"batch_size\"]\n",
    "    history[\"epochs_total\"] = params[\"epochs\"]\n",
    "    history[\"mejor_epoch\"] = best_epoch\n",
    "    history[\"mejor_val_accuracy\"] = best_val_accuracy\n",
    "\n",
    "    history.to_csv(filepath, index=False)\n",
    "    print(f\"✅ Resultados completos guardados en {filepath}\")\n",
    "\n",
    "    resultados_globales.append({\n",
    "        \"modelo\": nombre_modelo,\n",
    "        \"experimento\": exp_name,\n",
    "        \"lr\": params[\"learning_rate\"],\n",
    "        \"batch_size\": params[\"batch_size\"],\n",
    "        \"mejor_epoch\": best_epoch,\n",
    "        \"mejor_val_accuracy\": best_val_accuracy\n",
    "    })\n",
    "\n",
    "# ============================================\n",
    "# 6. Resumen consolidado de mejores resultados\n",
    "# ============================================\n",
    "def mostrar_resumen():\n",
    "    df_resumen = pd.DataFrame(resultados_globales)\n",
    "    print(\"\\n📈 RESUMEN CONSOLIDADO:\")\n",
    "    print(df_resumen)\n",
    "    return df_resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecución consolidada de la segunda versión del pipeline.\n",
    "# Entrena y evalúa las tres variantes MLP sobre los splits definidos anteriormente.\n",
    "\n",
    "# ============================================\n",
    "# 🚀 Entrenamiento de los 3 modelos con dataset en inglés\n",
    "# ============================================\n",
    "\n",
    "# Parámetros generales\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"🔧 Dispositivo de entrenamiento: {device}\")\n",
    "\n",
    "# Tamaño de vocabulario (grande: impacto en memoria y convergencia de embedding)\n",
    "vocab_size = tokenizer.vocab_size\n",
    "print(f\"📚 Tamaño del vocabulario: {vocab_size}\")\n",
    "\n",
    "# Modelos disponibles (embedding_dim por defecto = 128)\n",
    "modelos = {\n",
    "    \"RedNeuronalSimple\": lambda: RedNeuronalSimple(vocab_size),\n",
    "    \"RedNeuronalIntermedia\": lambda: RedNeuronalIntermedia(vocab_size),\n",
    "    \"RedNeuronalAvanzada\": lambda: RedNeuronalAvanzada(vocab_size),\n",
    "}\n",
    "\n",
    "# Configuraciones experimentales (epochs exp_2 = 400 aquí vs 500 en versión 1)\n",
    "experimentos = {\n",
    "    \"exp_1\": {\"learning_rate\": 0.001, \"epochs\": 100, \"batch_size\": 1024},\n",
    "    \"exp_2\": {\"learning_rate\": 0.0005, \"epochs\": 400, \"batch_size\": 2048},\n",
    "}\n",
    "\n",
    "# Loop principal sobre modelos y configuraciones\n",
    "for nombre_modelo, modelo_fn in modelos.items():\n",
    "    for exp_name, params in experimentos.items():\n",
    "        print(f\"\\n🚀 Entrenando {nombre_modelo} - {exp_name}\")\n",
    "\n",
    "        # DataLoaders (shuffle solo en train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True)\n",
    "        val_loader   = DataLoader(validation_dataset, batch_size=params[\"batch_size\"])  # no shuffle\n",
    "        test_loader  = DataLoader(test_dataset, batch_size=params[\"batch_size\"])        # evaluación\n",
    "\n",
    "        # Instancia y envío a dispositivo\n",
    "        modelo = modelo_fn().to(device)\n",
    "        optimizer = optim.Adam(modelo.parameters(), lr=params[\"learning_rate\"])  # Sin weight decay ni scheduler\n",
    "        criterion = nn.CrossEntropyLoss()                                          # Pérdida multiclase estándar\n",
    "\n",
    "        # Entrenamiento con early stopping (paciencia fija = 25)\n",
    "        modelo, best_val_acc, best_epoch, history = train_model_es(\n",
    "            modelo, train_loader, val_loader, optimizer, criterion,\n",
    "            epochs=params[\"epochs\"], device=device, patience=25\n",
    "        )\n",
    "\n",
    "        # Guardar historial y metadatos\n",
    "        guardar_resultados(nombre_modelo, exp_name, params, best_epoch, best_val_acc, history)\n",
    "\n",
    "        # Evaluación exhaustiva en test (Accuracy + F1 + matriz de confusión)\n",
    "        evaluar_metricas(modelo, test_loader, criterion, device, nombre_modelo, exp_name)\n",
    "\n",
    "# Mostrar resumen consolidado final\n",
    "df_resumen = mostrar_resumen()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00487558d9014207a51a42c2cabf343a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0b519db5dc614d0d887b9caeb23d7af5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ffd206b6b32348d5abd59a9ee00cc9f9",
       "IPY_MODEL_f08087b41add4088be295f80bf402342",
       "IPY_MODEL_337b0494b0174065a0be5ae68f71810b"
      ],
      "layout": "IPY_MODEL_a902b7d10b664f21a2e59926b0271b47"
     }
    },
    "0b9f07efe09547e898f7cd8bdb512ae4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0d886fc50d1f4526bdf1dfd6f0effc20": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9ca105a16a9416e8eff1c16881cc47e",
      "placeholder": "​",
      "style": "IPY_MODEL_a6ca5a11b66f454d96c0f8d137b3ce26",
      "value": "Map: 100%"
     }
    },
    "102af68cb5e94a11bd1b5bac4ca7b226": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fd2ae367c2c42d097f24c13da029c89": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "258b525b6fb4414ca778cfb6f4a78127": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2b4c236665374af5a41f6a02442a0433": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a505b074e99f464f8bc386196d83ea87",
      "placeholder": "​",
      "style": "IPY_MODEL_258b525b6fb4414ca778cfb6f4a78127",
      "value": " 120000/120000 [00:43&lt;00:00, 2962.60 examples/s]"
     }
    },
    "2cd725340a8146b88465f66a8469002f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2f93d3b96972433588145aefea6ace26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32a32309df374ac6a24863006c2c6576": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "337b0494b0174065a0be5ae68f71810b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69879fd52738486cb4b1c698c1657583",
      "placeholder": "​",
      "style": "IPY_MODEL_992ba7781ad34fb2a8f69ccc1b638649",
      "value": " 972000/972000 [10:31&lt;00:00, 2463.18 examples/s]"
     }
    },
    "3aa53de1aab44be1be0249c4030d7826": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ba1b5a5ffa5444d9331dffc88092027": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b9f07efe09547e898f7cd8bdb512ae4",
      "placeholder": "​",
      "style": "IPY_MODEL_a7526e7f159b4a4c98bf1cbb8acec48f",
      "value": "Map: 100%"
     }
    },
    "3ed11a3f7b344580a6f02797b5778531": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "44258844ff6d4e54b4b95f26d6e6ee2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3ba1b5a5ffa5444d9331dffc88092027",
       "IPY_MODEL_d8a0297c325e48c0927284900e7c07a1",
       "IPY_MODEL_2b4c236665374af5a41f6a02442a0433"
      ],
      "layout": "IPY_MODEL_102af68cb5e94a11bd1b5bac4ca7b226"
     }
    },
    "69879fd52738486cb4b1c698c1657583": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b91197db3f14bc7948521a1f231dbc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0d886fc50d1f4526bdf1dfd6f0effc20",
       "IPY_MODEL_e70b0a1e53c14fc786c23d4064269d3a",
       "IPY_MODEL_ffdc4abe356e4f7d802e877e315239a5"
      ],
      "layout": "IPY_MODEL_00487558d9014207a51a42c2cabf343a"
     }
    },
    "7ebecc96e36c42528a79e7b643705889": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "992ba7781ad34fb2a8f69ccc1b638649": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bf92d372bd14d36ac9c2c73057ab848": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a505b074e99f464f8bc386196d83ea87": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6ca5a11b66f454d96c0f8d137b3ce26": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a7526e7f159b4a4c98bf1cbb8acec48f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a902b7d10b664f21a2e59926b0271b47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9ca105a16a9416e8eff1c16881cc47e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c8209025d8f54d0493d51a17d4f61243": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d083d45b9d01409fac5e25aecaf708cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8a0297c325e48c0927284900e7c07a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c8209025d8f54d0493d51a17d4f61243",
      "max": 120000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9bf92d372bd14d36ac9c2c73057ab848",
      "value": 120000
     }
    },
    "e70b0a1e53c14fc786c23d4064269d3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2f93d3b96972433588145aefea6ace26",
      "max": 108000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3ed11a3f7b344580a6f02797b5778531",
      "value": 108000
     }
    },
    "f08087b41add4088be295f80bf402342": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d083d45b9d01409fac5e25aecaf708cf",
      "max": 972000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1fd2ae367c2c42d097f24c13da029c89",
      "value": 972000
     }
    },
    "ffd206b6b32348d5abd59a9ee00cc9f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_32a32309df374ac6a24863006c2c6576",
      "placeholder": "​",
      "style": "IPY_MODEL_3aa53de1aab44be1be0249c4030d7826",
      "value": "Map: 100%"
     }
    },
    "ffdc4abe356e4f7d802e877e315239a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ebecc96e36c42528a79e7b643705889",
      "placeholder": "​",
      "style": "IPY_MODEL_2cd725340a8146b88465f66a8469002f",
      "value": " 108000/108000 [00:39&lt;00:00, 2883.66 examples/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
